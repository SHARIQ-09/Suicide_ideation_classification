{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "train_embeddings = torch.load('train_embeddings.pt').to(device)\n",
    "test_embeddings = torch.load('test_embeddings.pt').to(device)\n",
    "val_embeddings = torch.load('val_embeddings.pt').to(device)\n",
    "\n",
    "train_labels = torch.load('train_labels_tensor.pt').to(device)\n",
    "test_labels = torch.load('test_labels_tensor.pt').to(device)\n",
    "val_labels = torch.load('val_labels_tensor.pt').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape: torch.Size([185542, 775])\n",
      "Validation embeddings shape: torch.Size([23193, 775])\n",
      "Test embeddings shape: torch.Size([23193, 775])\n"
     ]
    }
   ],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "    norms = embeddings.norm(p=2, dim=1, keepdim=True)\n",
    "    normalized_embeddings = embeddings / norms\n",
    "    return normalized_embeddings\n",
    "\n",
    "# Normalize the embeddings\n",
    "train_embeddings= normalize_embeddings(train_embeddings).to(device)\n",
    "val_embeddings = normalize_embeddings(val_embeddings).to(device)\n",
    "test_embeddings = normalize_embeddings(test_embeddings).to(device)\n",
    "\n",
    "# Now the embeddings are normalized and ready for use\n",
    "print(\"Train embeddings shape:\", train_embeddings.shape)\n",
    "print(\"Validation embeddings shape:\", val_embeddings.shape)\n",
    "print(\"Test embeddings shape:\", test_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(train_embeddings, train_labels)\n",
    "val_dataset = TensorDataset(val_embeddings, val_labels)\n",
    "test_dataset = TensorDataset(test_embeddings, test_labels)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, bidirectional=True):\n",
    "        super(RNNWithAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.bidirectional_factor = 2 if bidirectional else 1\n",
    "\n",
    "        # Input dimension should match combined embedding size (775)\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        self.attention = nn.Linear(hidden_dim * self.bidirectional_factor, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * self.bidirectional_factor, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Debugging statement to ensure input shape is correct\n",
    "        #print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "        # The input `x` must be of shape (batch_size, sequence_length, input_dim)\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "\n",
    "        # Print shapes for debugging\n",
    "        #print(f\"rnn_out shape: {rnn_out.shape}\")\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.tanh(self.attention(rnn_out))\n",
    "\n",
    "        # Ensure attn_weights has the correct shape\n",
    "        #print(f\"attn_weights (before softmax) shape: {attn_weights.shape}\")\n",
    "\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "\n",
    "        # Ensure attn_weights is 3D for batch matrix multiplication\n",
    "        attn_weights = attn_weights.permute(0, 2, 1)\n",
    "        #print(f\"attn_weights (after softmax and unsqueeze) shape: {attn_weights.shape}\")\n",
    "        \n",
    "        attn_output = torch.bmm(attn_weights, rnn_out).squeeze(1)\n",
    "\n",
    "        # Final linear layer\n",
    "        output = self.fc(attn_output)\n",
    "        return self.sigmoid(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_dim = train_embeddings.shape[1]  # Assuming embeddings have shape (batch_size, sequence_length, embedding_dim)\n",
    "hidden_dim = 128\n",
    "output_dim = 1  # Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = RNNWithAttention(input_dim, hidden_dim, output_dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4 , weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 625.3525, Accuracy: 0.9152\n",
      "Training precesion : 0.9225454824595072 , Training recall : 0.9065204038749582 , Training F1 : 0.9144627425403554\n",
      "Epoch 1/10\n",
      "Validation Loss: 77.9371\n",
      "Accuracy: 0.9166 ,Validation Precision: 0.9225, Recall: 0.9065, F1 Score: 0.9145\n",
      "Epoch 2/10, Loss: 607.7400, Accuracy: 0.9183\n",
      "Training precesion : 0.9253675745557333 , Training recall : 0.9101410545144988 , Training F1 : 0.9176911584951786\n",
      "Epoch 2/10\n",
      "Validation Loss: 76.4868\n",
      "Accuracy: 0.9174 ,Validation Precision: 0.9254, Recall: 0.9101, F1 Score: 0.9177\n",
      "Epoch 3/10, Loss: 598.2844, Accuracy: 0.9196\n",
      "Training precesion : 0.9257809510272362 , Training recall : 0.9123931854182605 , Training F1 : 0.9190383154238576\n",
      "Epoch 3/10\n",
      "Validation Loss: 75.3769\n",
      "Accuracy: 0.9189 ,Validation Precision: 0.9258, Recall: 0.9124, F1 Score: 0.9190\n",
      "Epoch 4/10, Loss: 591.1277, Accuracy: 0.9203\n",
      "Training precesion : 0.925652302017744 , Training recall : 0.9140418745487656 , Training F1 : 0.9198104512085362\n",
      "Epoch 4/10\n",
      "Validation Loss: 74.6977\n",
      "Accuracy: 0.9199 ,Validation Precision: 0.9257, Recall: 0.9140, F1 Score: 0.9198\n",
      "Epoch 5/10, Loss: 586.0234, Accuracy: 0.9213\n",
      "Training precesion : 0.926278032772587 , Training recall : 0.9155181517440545 , Training F1 : 0.9208666623311873\n",
      "Epoch 5/10\n",
      "Validation Loss: 74.0568\n",
      "Accuracy: 0.9203 ,Validation Precision: 0.9263, Recall: 0.9155, F1 Score: 0.9209\n",
      "Epoch 6/10, Loss: 582.1879, Accuracy: 0.9218\n",
      "Training precesion : 0.9260307370804127 , Training recall : 0.916800465512225 , Training F1 : 0.9213924852580452\n",
      "Epoch 6/10\n",
      "Validation Loss: 73.6645\n",
      "Accuracy: 0.9206 ,Validation Precision: 0.9260, Recall: 0.9168, F1 Score: 0.9214\n",
      "Epoch 7/10, Loss: 579.2105, Accuracy: 0.9222\n",
      "Training precesion : 0.9262913964924496 , Training recall : 0.9174577860152369 , Training F1 : 0.9218534298413248\n",
      "Epoch 7/10\n",
      "Validation Loss: 73.6753\n",
      "Accuracy: 0.9216 ,Validation Precision: 0.9263, Recall: 0.9175, F1 Score: 0.9219\n",
      "Epoch 8/10, Loss: 576.6517, Accuracy: 0.9227\n",
      "Training precesion : 0.9262817029969791 , Training recall : 0.9185461363562892 , Training F1 : 0.9223977016345014\n",
      "Epoch 8/10\n",
      "Validation Loss: 73.0453\n",
      "Accuracy: 0.9217 ,Validation Precision: 0.9263, Recall: 0.9185, F1 Score: 0.9224\n",
      "Epoch 9/10, Loss: 574.2493, Accuracy: 0.9230\n",
      "Training precesion : 0.9267722612977921 , Training recall : 0.9186431180698483 , Training F1 : 0.9226897850509773\n",
      "Epoch 9/10\n",
      "Validation Loss: 73.0584\n",
      "Accuracy: 0.9222 ,Validation Precision: 0.9268, Recall: 0.9186, F1 Score: 0.9227\n",
      "Epoch 10/10, Loss: 574.0480, Accuracy: 0.9232\n",
      "Training precesion : 0.926793223727819 , Training recall : 0.9190633721619379 , Training F1 : 0.9229121128833294\n",
      "Epoch 10/10\n",
      "Validation Loss: 72.6952\n",
      "Accuracy: 0.9228 ,Validation Precision: 0.9268, Recall: 0.9191, F1 Score: 0.9229\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds=[]\n",
    "    all_labels=[]\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        # Reshape inputs to add the sequence dimension, if not already present\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "    \n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    print (f'Training precesion : {precision} , Training recall : {recall} , Training F1 : {f1}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_preds=[]\n",
    "    val_labels=[]\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs=inputs.unsqueeze(1)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            val_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            val_preds.extend(predicted.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_accuracy = correct / total\n",
    "\n",
    "    val_precision = precision_score(all_labels, all_preds)\n",
    "    val_recall = recall_score(all_labels, all_preds)\n",
    "    val_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    #val_precision, val_recall, val_f1 = evaluate_model(model, val_loader, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f'Accuracy: {val_accuracy:.4f} ,Validation Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1 Score: {val_f1:.4f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 70.2676\n",
      "Test Accuracy: 0.9244,Test Precision: 0.9245, Recall: 0.9244, F1 Score: 0.9244\n"
     ]
    }
   ],
   "source": [
    "# Testing step\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds=[]\n",
    "all_labels=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs=inputs.unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        test_loss += loss.item()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = correct / total\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "test_precision = precision_score(all_labels, all_preds)\n",
    "test_recall = recall_score(all_labels, all_preds)\n",
    "test_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "# Test evaluation\n",
    "#test_precision, test_recall, test_f1 = evaluate_model(model, test_loader, device)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f},Test Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1 Score: {test_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
