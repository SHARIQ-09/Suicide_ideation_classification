{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.utils as nn_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined_embeddings_tensor = torch.load('train_embeddings.pt')\n",
    "val_combined_embeddings_tensor = torch.load('val_embeddings.pt')\n",
    "test_combined_embeddings_tensor = torch.load('test_embeddings.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the train embeddings\n",
    "train_combined_embeddings_tensor = train_combined_embeddings_tensor.unsqueeze(1)  # Shape: [185659, 1, 775]\n",
    "\n",
    "# Assuming val_combined_embeddings_tensor is your validation embeddings tensor\n",
    "val_combined_embeddings_tensor = val_combined_embeddings_tensor.unsqueeze(1)  # Shape: [val_size, 1, 775]\n",
    "\n",
    "# Assuming test_combined_embeddings_tensor is your test embeddings tensor\n",
    "test_combined_embeddings_tensor = test_combined_embeddings_tensor.unsqueeze(1)  # Shape: [test_size, 1, 775]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your tensors here or load them from disk\n",
    "train_labels_tensor = torch.load('train_labels_tensor.pt')\n",
    "val_labels_tensor = torch.load('val_labels_tensor.pt')\n",
    "test_labels_tensor = torch.load('test_labels_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_combined_embeddings_tensor, train_labels_tensor)\n",
    "val_dataset = TensorDataset(val_combined_embeddings_tensor, val_labels_tensor)\n",
    "test_dataset = TensorDataset(test_combined_embeddings_tensor, test_labels_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_labels(labels):\n",
    "    return labels.float().unsqueeze(1)  # Change shape from [batch_size] to [batch_size, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        all_labels=[]\n",
    "        all_preds=[]\n",
    "        for batch in train_loader:\n",
    "            embeddings, labels = batch\n",
    "            embeddings , labels = embeddings.to(device),adjust_labels(labels).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(embeddings).float()\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and labels for metric calculation\n",
    "            preds = (outputs > 0.5).float()\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "\n",
    "        train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        train_precision = precision_score(all_labels, all_preds)\n",
    "        train_recall = recall_score(all_labels, all_preds)\n",
    "        train_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "        # Print metrics for training\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader)}')\n",
    "        print(f'Training Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1 Score: {train_f1:.4f}')\n",
    "              \n",
    "        # Print the average training loss for this epoch\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader)}')\n",
    "        \n",
    "        # Validate the model\n",
    "        validate_model(model, val_loader, criterion,epoch)\n",
    "\n",
    "# Function to validate the model\n",
    "def validate_model(model, val_loader, criterion,epoch):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            embeddings, labels = batch\n",
    "            embeddings , labels = embeddings.to(device) ,adjust_labels(labels).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(embeddings).float()\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and labels for metric calculation\n",
    "            preds = (outputs > 0.5).float()\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "    # Calculate metrics\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "\n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    val_precision = precision_score(all_labels, all_preds)\n",
    "    val_recall = recall_score(all_labels, all_preds)\n",
    "    val_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    # Print metrics for validation\n",
    "    print(f'Validation Loss: {val_loss / len(val_loader)}')\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1 Score: {val_f1:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TransformerDecoderClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_dim=768, num_decoder_layers=6, dropout=0.1):\n",
    "        super(TransformerDecoderClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "         # Linear layer to project embeddings to the desired dimension\n",
    "        self.projection = nn.Linear(775, self.embedding_dim)\n",
    "        \n",
    "        # Define the Transformer decoder layer\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=8, dropout=dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        \n",
    "        # Final linear layer for classification\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "        # Sigmoid for binary classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, embedding_dim]\n",
    "        # Project embeddings to the desired dimension\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        # Apply the Transformer decoder\n",
    "        x = self.transformer_decoder(x, x)  # tgt and memory both are the input in this case\n",
    "        \n",
    "        # Apply the final linear layer and sigmoid activation\n",
    "        x = self.fc(x[:, -1, :])  # Use the output from the last token\n",
    "        \n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerDecoderClassifier(\n",
    "    num_classes=1,  # Binary classification\n",
    "    embedding_dim=768,  # Updated embedding dimension\n",
    "    num_decoder_layers=6,  # Number of decoder layers\n",
    "    dropout=0.1  # Dropout rate\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)  # Adjust learning rate as needed\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\Lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.21399440768875527\n",
      "Training Accuracy: 0.9164, Precision: 0.9168, Recall: 0.9159, F1 Score: 0.9164\n",
      "Epoch [1/10], Loss: 0.21399440768875527\n",
      "Validation Loss: 0.20831699556831657\n",
      "Validation Accuracy: 0.9212, Precision: 0.9024, Recall: 0.9447, F1 Score: 0.9231\n",
      "Epoch [2/10], Loss: 0.1991200460470181\n",
      "Training Accuracy: 0.9226, Precision: 0.9225, Recall: 0.9228, F1 Score: 0.9226\n",
      "Epoch [2/10], Loss: 0.1991200460470181\n",
      "Validation Loss: 0.19575665648641258\n",
      "Validation Accuracy: 0.9235, Precision: 0.9138, Recall: 0.9353, F1 Score: 0.9244\n",
      "Epoch [3/10], Loss: 0.1954694988768452\n",
      "Training Accuracy: 0.9245, Precision: 0.9244, Recall: 0.9247, F1 Score: 0.9245\n",
      "Epoch [3/10], Loss: 0.1954694988768452\n",
      "Validation Loss: 0.19789972008302295\n",
      "Validation Accuracy: 0.9251, Precision: 0.9142, Recall: 0.9383, F1 Score: 0.9261\n",
      "Epoch [4/10], Loss: 0.19243386905344217\n",
      "Training Accuracy: 0.9257, Precision: 0.9257, Recall: 0.9257, F1 Score: 0.9257\n",
      "Epoch [4/10], Loss: 0.19243386905344217\n",
      "Validation Loss: 0.19378130447247932\n",
      "Validation Accuracy: 0.9254, Precision: 0.9126, Recall: 0.9409, F1 Score: 0.9265\n",
      "Epoch [5/10], Loss: 0.190034271147379\n",
      "Training Accuracy: 0.9268, Precision: 0.9264, Recall: 0.9273, F1 Score: 0.9268\n",
      "Epoch [5/10], Loss: 0.190034271147379\n",
      "Validation Loss: 0.19652921696675235\n",
      "Validation Accuracy: 0.9256, Precision: 0.9115, Recall: 0.9428, F1 Score: 0.9269\n",
      "Epoch [6/10], Loss: 0.18849128418911848\n",
      "Training Accuracy: 0.9273, Precision: 0.9270, Recall: 0.9278, F1 Score: 0.9274\n",
      "Epoch [6/10], Loss: 0.18849128418911848\n",
      "Validation Loss: 0.1918967213610123\n",
      "Validation Accuracy: 0.9264, Precision: 0.9213, Recall: 0.9324, F1 Score: 0.9268\n",
      "Epoch [7/10], Loss: 0.18605761001071894\n",
      "Training Accuracy: 0.9282, Precision: 0.9281, Recall: 0.9285, F1 Score: 0.9283\n",
      "Epoch [7/10], Loss: 0.18605761001071894\n",
      "Validation Loss: 0.18913681071893923\n",
      "Validation Accuracy: 0.9267, Precision: 0.9240, Recall: 0.9299, F1 Score: 0.9270\n",
      "Epoch [8/10], Loss: 0.18433274995971805\n",
      "Training Accuracy: 0.9290, Precision: 0.9291, Recall: 0.9289, F1 Score: 0.9290\n",
      "Epoch [8/10], Loss: 0.18433274995971805\n",
      "Validation Loss: 0.19142060127494664\n",
      "Validation Accuracy: 0.9271, Precision: 0.9179, Recall: 0.9383, F1 Score: 0.9280\n",
      "Epoch [9/10], Loss: 0.18269516540308064\n",
      "Training Accuracy: 0.9296, Precision: 0.9296, Recall: 0.9297, F1 Score: 0.9296\n",
      "Epoch [9/10], Loss: 0.18269516540308064\n",
      "Validation Loss: 0.19097583022353978\n",
      "Validation Accuracy: 0.9254, Precision: 0.9409, Recall: 0.9078, F1 Score: 0.9241\n",
      "Epoch [10/10], Loss: 0.1809727655953992\n",
      "Training Accuracy: 0.9306, Precision: 0.9310, Recall: 0.9302, F1 Score: 0.9306\n",
      "Epoch [10/10], Loss: 0.1809727655953992\n",
      "Validation Loss: 0.19340381442986684\n",
      "Validation Accuracy: 0.9240, Precision: 0.9352, Recall: 0.9111, F1 Score: 0.9230\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming you have the model, optimizer, criterion, and data loaders ready:\n",
    "num_epochs = 10  # Adjust based on your dataset size and needs\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # No gradient calculation during testing\n",
    "        for embeddings, labels in test_loader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            labels = adjust_labels(labels).to(device).float()  # Convert labels to float if using BCELoss\n",
    "\n",
    "\n",
    "            outputs = model(embeddings)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Assuming outputs and labels are in the range [0, 1]\n",
    "            preds = (outputs > 0.5).float()\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate average test loss\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    print(f'Test Loss: {test_loss}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "\n",
    "    return test_loss, accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1849774606294673\n",
      "Accuracy: 0.9285991462941404\n",
      "Precision: 0.9424274781989678\n",
      "Recall: 0.9130172413793104\n",
      "F1 Score: 0.9274892722655224\n"
     ]
    }
   ],
   "source": [
    "# Assuming test_loader is defined, and you have the model and criterion set up\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Testing the model\n",
    "test_loss, accuracy, precision, recall, f1 = test_model(model, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
